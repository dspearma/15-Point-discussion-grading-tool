import streamlit as st

import pandas as pd

import io

import sys

import os

import requests

import csv

import json

import re

import time

import math

from typing import Dict, Any, List, Union

from difflib import SequenceMatcher

import plotly.express as px



# Load environment variables

try:

Â  Â  from dotenv import load_dotenv

Â  Â  load_dotenv()

except ImportError:

Â  Â  st.warning("python-dotenv not installed. API key must be entered manually.")



# ============================================

# ALL FUNCTION DEFINITIONS GO HERE

# ============================================



def validate_api_key(api_key: str) -> bool:

Â  Â  """Validate the OpenRouter API key format."""

Â  Â  if not api_key:

Â  Â  Â  Â  return False

Â  Â  return api_key.startswith("sk-or-v1-") and len(api_key) > 20



def strip_tags(text: str) -> str:

Â  Â  """Removes HTML tags from a string."""

Â  Â  pattern = re.compile(r'<[^>]*>')

Â  Â  return re.sub(pattern, '', text)



def fix_encoding(text: Union[str, bytes]) -> str:

Â  Â  """Fix common encoding issues and replace problematic characters."""

Â  Â  if not isinstance(text, (str, bytes)):

Â  Â  Â  Â  return str(text)



Â  Â  if isinstance(text, bytes):

Â  Â  Â  Â  text = text.decode('utf-8', errors='ignore')



Â  Â  # Common encoding fixes for smart quotes/dashes/html entities

Â  Â  replacements = {

Â  Â  Â  Â  'Ã¢â‚¬â„¢': "'", 'Ã¢â‚¬Å“': '"', 'Ã¢â‚¬': '"', 'Ã¢â‚¬"': 'â€“', 'Ã¢â‚¬"': 'â€”',

Â  Â  Â  Â  '\u201c': '"', '\u201d': '"', '\u2018': "'", '\u2019': "'",

Â  Â  Â  Â  '\u2013': '-', '\u2014': 'â€”',

Â  Â  Â  Â  'Ã¢â‚¬"': 'â€”',

Â  Â  Â  Â  '&nbsp;': ' ',

Â  Â  Â  Â  '&quot;': '"'

Â  Â  }



Â  Â  for original, replacement in replacements.items():

Â  Â  Â  Â  text = text.replace(original, replacement)



Â  Â  try:

Â  Â  Â  Â  text = text.encode('utf-8', errors='ignore').decode('utf-8')

Â  Â  except Exception:

Â  Â  Â  Â  pass



Â  Â  return text



def recursively_clean(value: Any) -> Any:

Â  Â  """Recursively clean all string values in a data structure using fix_encoding."""

Â  Â  if isinstance(value, str):

Â  Â  Â  Â  return fix_encoding(value)

Â  Â  elif isinstance(value, dict):

Â  Â  Â  Â  return {k: recursively_clean(v) for k, v in value.items()}

Â  Â  elif isinstance(value, list):

Â  Â  Â  Â  return [recursively_clean(item) for item in value]

Â  Â  else:

Â  Â  Â  Â  return value



def round_nearest_half(value: float) -> float:

Â  Â  """Round a float to the nearest 0.5 increment."""

Â  Â  return round(value * 2) / 2



def retry_with_backoff(func, max_retries: int = 3, base_delay: int = 5, max_delay: int = 30):

Â  Â  """Retry function with exponential backoff."""

Â  Â  for attempt in range(max_retries + 1):

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  return func()

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  if attempt == max_retries:

Â  Â  Â  Â  Â  Â  Â  Â  raise e



Â  Â  Â  Â  Â  Â  delay = min(base_delay * (2 ** attempt), max_delay)

Â  Â  Â  Â  Â  Â  time.sleep(delay)



def robust_api_call(api_url: str, headers: Dict, payload: Dict, timeout: int = 60, max_retries: int = 3, api_key: str = None) -> str:

Â  Â  """Make API call with retries and proper error handling."""

Â  Â  def api_call():

Â  Â  Â  Â  # Check if API key is provided

Â  Â  Â  Â  if not api_key:

Â  Â  Â  Â  Â  Â  raise ValueError("No API key provided")

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Ensure the API key is properly formatted

Â  Â  Â  Â  if not api_key.startswith("sk-or-v1-"):

Â  Â  Â  Â  Â  Â  raise ValueError(f"Invalid API key format. Expected format: sk-or-v1-...")

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Create a new headers dictionary to avoid any potential issues

Â  Â  Â  Â  new_headers = {

Â  Â  Â  Â  Â  Â  "Authorization": f"Bearer {api_key}",

Â  Â  Â  Â  Â  Â  "Content-Type": "application/json",

Â  Â  Â  Â  Â  Â  "HTTP-Referer": "https://your-app-url.com",

Â  Â  Â  Â  Â  Â  "X-Title": "Discussion Grading Tool"

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Make the API call

Â  Â  Â  Â  response = requests.post(api_url, headers=new_headers, json=payload, timeout=timeout)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Handle specific HTTP errors

Â  Â  Â  Â  if response.status_code == 401:

Â  Â  Â  Â  Â  Â  raise ValueError("Authentication failed. Please check your API key.")

Â  Â  Â  Â  elif response.status_code == 429:

Â  Â  Â  Â  Â  Â  raise ValueError("Rate limit exceeded. Please try again later.")

Â  Â  Â  Â  elif response.status_code >= 500:

Â  Â  Â  Â  Â  Â  raise ValueError(f"Server error: {response.status_code}")

Â  Â  Â  Â Â 

Â  Â  Â  Â  response.raise_for_status()

Â  Â  Â  Â  result = response.json()



Â  Â  Â  Â  if not result.get("choices"):

Â  Â  Â  Â  Â  Â  raise ValueError("Invalid API response format - missing choices")



Â  Â  Â  Â  if result["choices"][0]["message"]["content"] is None:

Â  Â  Â  Â  Â  Â  Â  Â  Â raise ValueError("Empty content block received from API.")



Â  Â  Â  Â  grade_response = result["choices"][0]["message"]["content"]

Â  Â  Â  Â  if not grade_response.strip():

Â  Â  Â  Â  Â  Â  raise ValueError("Empty API response")



Â  Â  Â  Â  return grade_response



Â  Â  return retry_with_backoff(api_call, max_retries=max_retries)



def robust_json_parsing(response_text: str, max_retries: int = 2) -> Dict:

Â  Â  """Parse clean JSON, primarily expecting the LLM to follow the format."""

Â  Â  def parse_json():

Â  Â  Â  Â  response_text_clean = fix_encoding(response_text.strip())



Â  Â  Â  Â  # Clean common LLM formatting issues (code fences)

Â  Â  Â  Â  response_text_clean = re.sub(r'^```json\s*', '', response_text_clean, flags=re.DOTALL)

Â  Â  Â  Â  response_text_clean = re.sub(r'\s*```$', '', response_text_clean, flags=re.DOTALL)

Â  Â  Â  Â  response_text_clean = re.sub(r'^```\s*', '', response_text_clean, flags=re.DOTALL)



Â  Â  Â  Â  # Attempt standard JSON loading

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  return json.loads(response_text_clean)

Â  Â  Â  Â  except json.JSONDecodeError as e:

Â  Â  Â  Â  Â  Â  # Use regex to find the likely JSON object

Â  Â  Â  Â  Â  Â  json_match = re.search(r'\{.*\}', response_text_clean, re.DOTALL)

Â  Â  Â  Â  Â  Â  if json_match:

Â  Â  Â  Â  Â  Â  Â  Â  extracted_json = json_match.group(0)

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return json.loads(extracted_json)

Â  Â  Â  Â  Â  Â  Â  Â  except json.JSONDecodeError as e2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError(f"Could not parse valid JSON even after extraction: {str(e2)}")



Â  Â  Â  Â  Â  Â  raise ValueError(f"Could not find or parse valid JSON structure: {str(e)}")



Â  Â  return retry_with_backoff(parse_json, max_retries=max_retries)



def extract_section_robust(text: str, section_name: str, alternative_names: List[str] = None, stop_indicators: List[str] = None) -> str:

Â  Â  """

Â  Â  Robust section extraction with better boundary detection.

Â  Â  Correctly handles headers followed by content on the same line.

Â  Â  """

Â  Â  alternative_names = alternative_names or []

Â  Â  stop_indicators = stop_indicators or ['Discussion', 'Reading', 'Video', 'Key Terms', 'Objective', 'Key Concepts', 'Materials', 'Prompt']



Â  Â  all_names = [section_name] + alternative_names

Â  Â  content_lines = []

Â  Â  in_section = False



Â  Â  lines = text.split('\n')



Â  Â  for i, line in enumerate(lines):

Â  Â  Â  Â  line_clean = line.strip()



Â  Â  Â  Â  # Check for start of section

Â  Â  Â  Â  is_header = False



Â  Â  Â  Â  if not in_section:

Â  Â  Â  Â  Â  Â  for name in all_names:

Â  Â  Â  Â  Â  Â  Â  Â  name_escaped = re.escape(name)

Â  Â  Â  Â  Â  Â  Â  Â  pattern = r'^\s*' + name_escaped + r'[:\s]*(.*)$'

Â  Â  Â  Â  Â  Â  Â  Â  match_start = re.match(pattern, line_clean, re.IGNORECASE)



Â  Â  Â  Â  Â  Â  Â  Â  if match_start:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  in_section = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_header = True



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  potential_content = match_start.group(1).strip()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if potential_content:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content_lines.append(potential_content)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  if is_header:

Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  if in_section:

Â  Â  Â  Â  Â  Â  # Check for stop condition

Â  Â  Â  Â  Â  Â  is_stop_indicator = False

Â  Â  Â  Â  Â  Â  for indicator in stop_indicators:

Â  Â  Â  Â  Â  Â  Â  Â  if re.match(r'^\s*' + re.escape(indicator) + r'[:\s]*', line_clean, re.IGNORECASE) and len(line_clean.split()) < 8:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_stop_indicator = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  Â  Â  if is_stop_indicator and line_clean:

Â  Â  Â  Â  Â  Â  Â  Â  in_section = False

Â  Â  Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  Â  Â  # Continue collecting content

Â  Â  Â  Â  Â  Â  if line_clean:

Â  Â  Â  Â  Â  Â  Â  Â  content_lines.append(line_clean)



Â  Â  if not content_lines:

Â  Â  Â  Â  return ""



Â  Â  content = '\n'.join(content_lines).strip()

Â  Â  return content



def parse_lesson_plan_comprehensive(docx_content: bytes):

Â  Â  """Comprehensive lesson plan parser from DOCX content."""

Â  Â  try:

Â  Â  Â  Â  from docx import Document

Â  Â  except ImportError:

Â  Â  Â  Â  raise ImportError("python-docx library required. Please install it with: pip install python-docx")



Â  Â  docx_io = io.BytesIO(docx_content)

Â  Â  document = Document(docx_io)



Â  Â  # Extract all text

Â  Â  full_text = "\n".join([p.text.strip() for p in document.paragraphs if p.text.strip()])

Â  Â  full_text_clean = fix_encoding(full_text)



Â  Â  # Define sections and their required alternate names/stop indicators dynamically

Â  Â  sections_to_extract = {

Â  Â  Â  Â  "Discussion Prompt": (["Discussion", "Prompt", "Discussion Question", "Question"], ['Reading', 'Video', 'Key Terms', 'Objective', 'Key Concepts', 'Materials']),

Â  Â  Â  Â  "Reading": (["Assigned Reading", "Required Reading", "READING"], ['Discussion', 'Video', 'Key Terms', 'Objective', 'Key Concepts', 'Materials', 'Prompt']),

Â  Â  Â  Â  "Video": (["VIDEO", "Assigned Video", "Required Video"], ['Discussion', 'Reading', 'Key Terms', 'Objective', 'Key Concepts', 'Materials', 'Prompt']),

Â  Â  Â  Â  "Objective": (["Learning Objective", "Goals"], ['Discussion', 'Reading', 'Video', 'Key Terms', 'Objective', 'Materials', 'Prompt']),

Â  Â  Â  Â  "Key Concepts": (["Concepts", "Main Concepts", "KEY CONCEPTS"], ['Discussion', 'Reading', 'Video', 'Key Terms', 'Objective', 'Materials', 'Prompt']),

Â  Â  Â  Â  "Key Terms": (["KEY TERMS", "Terms", "Vocabulary"], ['Discussion', 'Reading', 'Video', 'Objective', 'Key Concepts', 'Materials', 'Prompt'])

Â  Â  }



Â  Â  parsed_sections = {}

Â  Â  for name, (alts, stops) in sections_to_extract.items():

Â  Â  Â  Â  parsed_sections[name] = extract_section_robust(full_text_clean, name, alts, stops)



Â  Â  # --- Key Terms Specific Parsing ---

Â  Â  key_terms_str = parsed_sections["Key Terms"]

Â  Â  key_terms = []



Â  Â  if key_terms_str:

Â  Â  Â  Â  key_terms_clean = re.sub(r'[\t\-\*\â€¢]', '\n', key_terms_str)

Â  Â  Â  Â  terms_list = re.split(r',\s*|\n', key_terms_clean)



Â  Â  Â  Â  for term in terms_list:

Â  Â  Â  Â  Â  Â  term = term.strip()

Â  Â  Â  Â  Â  Â  term = re.sub(r'^[\d\.\s\-\)]+', '', term)

Â  Â  Â  Â  Â  Â  term = re.sub(r'[\.,;\s\-]+$', '', term)



Â  Â  Â  Â  Â  Â  if term and len(term) > 2:

Â  Â  Â  Â  Â  Â  Â  Â  key_terms.append(term)



Â  Â  unique_key_terms = list(set([term for term in key_terms if len(term) > 2]))

Â  Â  unique_key_terms.sort()



Â  Â  # --- Fallback Discussion Prompt ---

Â  Â  discussion_prompt = parsed_sections["Discussion Prompt"]

Â  Â  if not discussion_prompt.strip():

Â  Â  Â  Â  # Fallback 1: Use Objective

Â  Â  Â  Â  if parsed_sections["Objective"].strip():

Â  Â  Â  Â  Â  Â  discussion_prompt = parsed_sections["Objective"]

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  # Fallback 2: Look for questions in the main text

Â  Â  Â  Â  Â  Â  question_match = re.search(r'([A-Za-z\s]+[?])', full_text_clean[-1000:], re.DOTALL)

Â  Â  Â  Â  Â  Â  if question_match:

Â  Â  Â  Â  Â  Â  Â  Â  discussion_prompt = question_match.group(1).strip()



Â  Â  return (

Â  Â  Â  Â  discussion_prompt,

Â  Â  Â  Â  parsed_sections["Reading"],

Â  Â  Â  Â  parsed_sections["Video"],

Â  Â  Â  Â  unique_key_terms,

Â  Â  Â  Â  parsed_sections["Objective"],

Â  Â  Â  Â  parsed_sections["Key Concepts"]

Â  Â  )



def normalize_text_for_matching(text: str) -> str:

Â  Â  if not text: return ""

Â  Â  text = strip_tags(text)

Â  Â  text = re.sub(r'[*_`#\-\,.:;]', ' ', text)

Â  Â  text = re.sub(r'\s+', ' ', text).strip().lower()

Â  Â  return text



def advanced_term_variations(term: str) -> List[str]:

Â  Â  term_clean = normalize_text_for_matching(term)

Â  Â  variations = set([term_clean])



Â  Â  if not term_clean or len(term_clean) < 3:

Â  Â  Â  Â  return []



Â  Â  if term_clean.endswith('s'):

Â  Â  Â  Â  variations.add(term_clean[:-1])

Â  Â  elif not term_clean.endswith('s'):

Â  Â  Â  Â  variations.add(term_clean + 's')



Â  Â  if '-' in term_clean:

Â  Â  Â  Â  variations.add(term_clean.replace('-', ' '))



Â  Â  return list(variations)



def detect_key_terms_presence(submission_text: str, key_terms: List[str]) -> List[str]:

Â  Â  if not key_terms or not submission_text:

Â  Â  Â  Â  return []



Â  Â  submission_norm = normalize_text_for_matching(submission_text)

Â  Â  detected_terms = []

Â  Â  detected_base_terms = set()



Â  Â  for term in key_terms:

Â  Â  Â  Â  if term in detected_base_terms: continue



Â  Â  Â  Â  term_variations = advanced_term_variations(term)



Â  Â  Â  Â  for variation in term_variations:

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  pattern = r'\b' + re.escape(variation) + r'\b'

Â  Â  Â  Â  Â  Â  Â  Â  if re.search(pattern, submission_norm):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detected_terms.append(term)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detected_base_terms.add(term)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  if len(variation.split()) == 1:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for submission_word in submission_norm.split():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if similarity_ratio(variation, submission_word) >= 0.9:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detected_terms.append(term)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detected_base_terms.add(term)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if term in detected_base_terms:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  except re.error:

Â  Â  Â  Â  Â  Â  Â  Â  continue



Â  Â  return list(set(detected_terms))



# EXACT Engagement Quality Analysis from your provided code

def analyze_engagement_quality(replies: List[str]) -> Dict[str, Any]:

Â  Â  replies = [fix_encoding(reply) for reply in replies]

Â  Â  valid_replies = [reply for reply in replies if reply and len(reply.split()) >= 10]

Â  Â  num_replies = len(valid_replies)



Â  Â  if num_replies == 0:

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  'score': 2.0,

Â  Â  Â  Â  Â  Â  'feedback': 'We encourage you to participate in peer discussion! Substantive replies engage with at least one classmate.',

Â  Â  Â  Â  Â  Â  'num_replies': 0,

Â  Â  Â  Â  Â  Â  'highest_quality_score': 2.0

Â  Â  Â  Â  }



Â  Â  reply_quality_scores = []



Â  Â  for reply in valid_replies:

Â  Â  Â  Â  reply_lower = reply.lower()

Â  Â  Â  Â  word_count = len(reply.split())

Â  Â  Â  Â  quality_score = 3.0



Â  Â  Â  Â  # Indicators of deep engagement (expanded and more flexible)

Â  Â  Â  Â  deep_engagement_indicators = [

Â  Â  Â  Â  Â  Â  'i disagree', 'i believe that', 'i believe it', 'i agree that', 'i understand',

Â  Â  Â  Â  Â  Â  'building on', 'contrary to', 'critique', 'elaborate', 'perspective', 'i appreciated',

Â  Â  Â  Â  Â  Â  'can you relate', 'in addition to', 'i think', 'surprised by', 'recommend',

Â  Â  Â  Â  Â  Â  'interesting point', 'you mentioned', 'your point about', 'i would argue',

Â  Â  Â  Â  Â  Â  'similar to what you said', 'expanding on', 'different perspective', 'your analysis',

Â  Â  Â  Â  Â  Â  'i noticed', 'as you pointed out', 'building upon', 'along those lines',

Â  Â  Â  Â  Â  Â  'your observation', 'another way to think', 'i wonder if', 'what if we consider'

Â  Â  Â  Â  ]



Â  Â  Â  Â  # Indicators of supporting arguments (expanded)

Â  Â  Â  Â  supporting_indicators = [

Â  Â  Â  Â  Â  Â  'because', 'however', 'although', 'critical', 'analysis', 'blueprint',

Â  Â  Â  Â  Â  Â  'therefore', 'moreover', 'furthermore', 'in contrast', 'similarly',

Â  Â  Â  Â  Â  Â  'for example', 'for instance', 'this suggests', 'this demonstrates',

Â  Â  Â  Â  Â  Â  'evidence shows', 'research indicates', 'as shown by', 'considering that'

Â  Â  Â  Â  ]



Â  Â  Â  Â  # Check for engagement indicators

Â  Â  Â  Â  has_deep_engagement = any(term in reply_lower for term in deep_engagement_indicators)

Â  Â  Â  Â  has_supporting_args = any(term in reply_lower for term in supporting_indicators)



Â  Â  Â  Â  # Check for question marks (asking thoughtful questions)

Â  Â  Â  Â  has_questions = '?' in reply



Â  Â  Â  Â  # Check for specific examples or concrete details (numbers, names, specific events)

Â  Â  Â  Â  has_specific_details = bool(re.search(r'\b\d+\b', reply)) or bool(re.search(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:argued|stated|mentioned|noted|wrote|said)', reply))



Â  Â  Â  Â  # Scoring logic - more generous and quality-focused

Â  Â  Â  Â  if word_count >= 75 and (has_deep_engagement or has_supporting_args or has_specific_details):

Â  Â  Â  Â  Â  Â  quality_score = 4.0

Â  Â  Â  Â  elif word_count >= 60 and (has_deep_engagement or has_supporting_args):

Â  Â  Â  Â  Â  Â  quality_score = 4.0

Â  Â  Â  Â  elif word_count >= 50 and has_deep_engagement and has_supporting_args:

Â  Â  Â  Â  Â  Â  quality_score = 4.0

Â  Â  Â  Â  elif word_count >= 50 and (has_deep_engagement or has_supporting_args or has_questions):

Â  Â  Â  Â  Â  Â  quality_score = 3.5

Â  Â  Â  Â  elif word_count >= 40 and (has_deep_engagement or has_supporting_args):

Â  Â  Â  Â  Â  Â  quality_score = 3.5

Â  Â  Â  Â  elif word_count >= 25:

Â  Â  Â  Â  Â  Â  quality_score = 3.0

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  quality_score = 3.0



Â  Â  Â  Â  quality_score = min(4.0, quality_score)

Â  Â  Â  Â  quality_score = round_nearest_half(quality_score)

Â  Â  Â  Â  reply_quality_scores.append(quality_score)



Â  Â  highest_quality_score = max(reply_quality_scores)



Â  Â  recipient_name = "a peer"

Â  Â  if valid_replies:

Â  Â  Â  Â  highest_quality_index = reply_quality_scores.index(highest_quality_score)

Â  Â  Â  Â  highest_quality_reply = valid_replies[highest_quality_index]



Â  Â  Â  Â  name_match = re.search(r'^(?:Hello|Hi|Dear|Hey|To)\s+([A-Za-z]+)\s*([A-Za-z]*),?', highest_quality_reply.strip(), re.IGNORECASE)



Â  Â  Â  Â  if name_match:

Â  Â  Â  Â  Â  Â  recipient_name = name_match.group(1).strip()

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  name_match_only = re.search(r'^([A-Za-z]+),', highest_quality_reply.strip())

Â  Â  Â  Â  Â  Â  if name_match_only:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â recipient_name = name_match_only.group(1).strip()



Â  Â  # Generate specific feedback for point deductions

Â  Â  if highest_quality_score >= 4.0:

Â  Â  Â  Â  feedback = f"Excellent engagement! Your reply to {recipient_name} demonstrates substantive interaction with their ideas, showing depth of analysis and critical thinking that meets the highest standards for peer discussion."

Â  Â  elif highest_quality_score >= 3.5:

Â  Â  Â  Â  feedback = f"Strong engagement. Your meaningful reply to {recipient_name} shows good interaction with their ideas. Your response demonstrates solid understanding and contributes meaningfully to the discussion. To earn full credit (4.0), consider adding more detailed analysis, specific examples, or deeper critical engagement with your peers' arguments."

Â  Â  elif highest_quality_score >= 3.0:

Â  Â  Â  Â  feedback = f"Adequate engagement. Your contribution meets the substantive length requirement. To strengthen future replies, consider adding more detailed analysis, specific examples, or deeper critical engagement with your peers' arguments. Points were deducted because your response lacked the depth of analysis expected for higher credit."

Â  Â  else:

Â  Â  Â  Â  feedback = 'Your participation meets the minimum requirement, but your replies lack substantive length or meaningful interaction. Focus on directly responding to and debating your peers\' ideas in detail. Points were deducted because your response was too brief or did not engage substantively with your peers\' ideas.'



Â  Â  return {

Â  Â  Â  Â  'score': highest_quality_score,

Â  Â  Â  Â  'feedback': feedback,

Â  Â  Â  Â  'num_replies': num_replies,

Â  Â  Â  Â  'highest_quality_score': highest_quality_score

Â  Â  }



def construct_final_feedback(

Â  Â  llm_results: Dict,

Â  Â  local_scores: Dict[str, float],

Â  Â  local_feedback: Dict[str, str],

Â  Â  improvement_areas: List[str],

Â  Â  student_first_name: str,

Â  Â  grading_scale: str

) -> str:

Â  Â  combined_prompt_key_score = local_scores['prompt_score'] + local_scores['key_terms_score']

Â  Â  prompt_feedback = llm_results.get('prompt_feedback', 'Feedback missing for prompt quality.')

Â  Â  key_terms_feedback = llm_results.get('key_terms_feedback', local_feedback.get('key_terms_fallback', 'Feedback missing for key terms.'))

Â  Â  reading_feedback = local_feedback.get('reading_feedback', llm_results.get('reading_feedback', 'Feedback missing for reading reference.'))

Â  Â  video_feedback = llm_results.get('video_feedback', 'Feedback missing for video reference.')

Â  Â  general_feedback_llm = llm_results.get('general_feedback', 'Overall submission quality was strong.')



Â  Â  engagement_feedback = local_feedback['engagement_feedback']



Â  Â  def transform_to_second_person(text):

Â  Â  Â  Â  if not text: return ""

Â  Â  Â  Â  # Fix the "you's" issue by replacing it with "you are"

Â  Â  Â  Â  text = re.sub(r'\byou\'s\b', 'you are', text, flags=re.IGNORECASE)

Â  Â  Â  Â  text = re.sub(r'\b(The student|This student|They|Their|He|His|She|Her)\b', lambda m: {'The student': 'You', 'This student': 'You', 'They': 'You', 'Their': 'Your', 'He': 'You', 'His': 'Your', 'She': 'You', 'Her': 'Your'}.get(m.group(1), m.group(1)), text, flags=re.IGNORECASE)

Â  Â  Â  Â  if text:

Â  Â  Â  Â  Â  Â  text = text.strip()

Â  Â  Â  Â  Â  Â  return text[0].upper() + text[1:]

Â  Â  Â  Â  return text



Â  Â  prompt_feedback = transform_to_second_person(prompt_feedback)

Â  Â  key_terms_feedback = transform_to_second_person(key_terms_feedback)

Â  Â  reading_feedback = transform_to_second_person(reading_feedback)

Â  Â  video_feedback = transform_to_second_person(video_feedback)

Â  Â  general_feedback_llm = transform_to_second_person(general_feedback_llm)



Â  Â  prompt_key_combined_feedback = f"{prompt_feedback.strip()} {key_terms_feedback.strip()}"

Â  Â Â 

Â  Â  # Format scores based on grading scale

Â  Â  if grading_scale == "15-point (3 categories)":

Â  Â  Â  Â  # Scale up the scores for 15-point scale (5 points per category)

Â  Â  Â  Â  # For 15-point scale, prompt and key terms are 2.5 points each

Â  Â  Â  Â  scaled_prompt_score = round_nearest_half(local_scores['prompt_score'] * 1.25)Â  # Scale from 2.0 to 2.5

Â  Â  Â  Â  scaled_key_terms_score = round_nearest_half(local_scores['key_terms_score'] * 1.25)Â  # Scale from 2.0 to 2.5

Â  Â  Â  Â  scaled_video_score = round_nearest_half(local_scores['video_score'] * 1.25)Â  # Scale from 4.0 to 5.0

Â  Â  Â  Â  scaled_reading_score = round_nearest_half(local_scores['reading_score'] * 1.25)Â  # Scale from 4.0 to 5.0

Â  Â  Â  Â Â 

Â  Â  Â  Â  prompt_key_formatted = f"PROMPT AND KEY TERMS [{scaled_prompt_score + scaled_key_terms_score:.1f}/5.0]: {prompt_key_combined_feedback}"

Â  Â  Â  Â  video_formatted = f"REFERENCE TO VIDEO [{scaled_video_score:.1f}]: {video_feedback}"

Â  Â  Â  Â  reading_formatted = f"REFERENCE TO READING [{scaled_reading_score:.1f}]: {reading_feedback}"

Â  Â  else:Â  # 16-point (4 categories)

Â  Â  Â  Â  prompt_key_formatted = f"PROMPT AND KEY TERMS [{combined_prompt_key_score:.1f}]: {prompt_key_combined_feedback}"

Â  Â  Â  Â  video_formatted = f"REFERENCE TO VIDEO [{local_scores['video_score']:.1f}]: {video_feedback}"

Â  Â  Â  Â  reading_formatted = f"REFERENCE TO READING [{local_scores['reading_score']:.1f}]: {reading_feedback}"

Â  Â  Â  Â  engagement_formatted = f"DISCUSSION ENGAGEMENT [{local_scores['engagement_score']:.1f}]: {engagement_feedback}"



Â  Â  if improvement_areas:

Â  Â  Â  Â  # Remove "Discussion Engagement" from improvement areas for 15-point scale

Â  Â  Â  Â  if grading_scale == "15-point (3 categories)" and "Discussion Engagement" in improvement_areas:

Â  Â  Â  Â  Â  Â  improvement_areas = [area for area in improvement_areas if area != "Discussion Engagement"]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  if improvement_areas:

Â  Â  Â  Â  Â  Â  improvement_focus = f"{student_first_name}, while your work demonstrates strong engagement with the content, focus on improving in the area(s) of: {', '.join(improvement_areas)} to maximize your synthesis of the concepts. {general_feedback_llm}"

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  improvement_focus = f"Outstanding work {student_first_name}! You excelled on all sections of this assignment. {general_feedback_llm}"

Â  Â  else:

Â  Â  Â  Â  improvement_focus = f"Outstanding work {student_first_name}! You excelled on all sections of this assignment. {general_feedback_llm}"



Â  Â  general_formatted = f"GENERAL FEEDBACK: {improvement_focus}"



Â  Â  # Construct final feedback based on grading scale

Â  Â  if grading_scale == "15-point (3 categories)":

Â  Â  Â  Â  final_feedback = '\n'.join([

Â  Â  Â  Â  Â  Â  prompt_key_formatted,

Â  Â  Â  Â  Â  Â  video_formatted,

Â  Â  Â  Â  Â  Â  reading_formatted,

Â  Â  Â  Â  Â  Â  general_formatted

Â  Â  Â  Â  ])

Â  Â  else:Â  # 16-point (4 categories)

Â  Â  Â  Â  final_feedback = '\n'.join([

Â  Â  Â  Â  Â  Â  prompt_key_formatted,

Â  Â  Â  Â  Â  Â  video_formatted,

Â  Â  Â  Â  Â  Â  reading_formatted,

Â  Â  Â  Â  Â  Â  engagement_formatted,

Â  Â  Â  Â  Â  Â  general_formatted

Â  Â  Â  Â  ])



Â  Â  final_feedback = re.sub(r'\s{2,}', ' ', final_feedback).strip()

Â  Â  return final_feedback



def similarity_ratio(str1, str2):

Â  Â  """Default fallback similarity function using SequenceMatcher."""

Â  Â  if not str1 or not str2:

Â  Â  Â  Â  return 0

Â  Â  return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()



def grade_submission_with_retries(

Â  Â  submission_text: str,

Â  Â  reading_text: str,

Â  Â  key_terms: List[str],

Â  Â  discussion_prompt: str,

Â  Â  student_first_name: str,

Â  Â  video_text: str,

Â  Â  replies: List[str],

Â  Â  api_key: str,

Â  Â  grading_scale: str

) -> Dict[str, str]:

Â  Â  """Grade submission with comprehensive local and API scoring."""



Â  Â  # 1. Local Scoring & Data Preparation

Â  Â  engagement_analysis = analyze_engagement_quality(replies)

Â  Â  engagement_score = engagement_analysis['score']

Â  Â  detected_terms = detect_key_terms_presence(submission_text, key_terms)

Â  Â  detected_terms_str = ', '.join(detected_terms) if detected_terms else 'none detected'

Â  Â  reading_info = {}



Â  Â  # --- Extract REQUIRED READING INFO ---

Â  Â  # The lesson plan will always say "READING: [reading details]"

Â  Â  # Extract the author and page numbers from the reading text



Â  Â  # First, check if "READING:" is in the text

Â  Â  if "READING:" in reading_text:

Â  Â  Â  Â  # Extract the full line that contains "READING:"

Â  Â  Â  Â  reading_line = ""

Â  Â  Â  Â  for line in reading_text.split('\n'):

Â  Â  Â  Â  Â  Â  if "READING:" in line:

Â  Â  Â  Â  Â  Â  Â  Â  reading_line = line.strip()

Â  Â  Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  # Extract author (first word after "READING:")

Â  Â  Â  Â  author_match = re.search(r'READING:\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', reading_line)

Â  Â  Â  Â  assigned_author = author_match.group(1).strip() if author_match else ""

Â  Â  else:

Â  Â  Â  Â  # Fallback if "READING:" is not explicitly in the text

Â  Â  Â  Â  # Extract author (first word before the first comma)

Â  Â  Â  Â  author_match = re.search(r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', reading_text.strip())

Â  Â  Â  Â  assigned_author = author_match.group(1).strip() if author_match else ""



Â  Â  # Look for page numbers in various formats

Â  Â  page_numbers = []

Â  Â Â 

Â  Â  # Try multiple patterns to extract page numbers

Â  Â  patterns = [

Â  Â  Â  Â  r'pages?\s+([\d.,\s-]+)',Â  # pages 81-83 or page 81

Â  Â  Â  Â  r'p\.?\s*([\d.,\s-]+)',Â  Â  # p.81 or p 81

Â  Â  Â  Â  r'([\d.,\s-]+)'Â  Â  Â  Â  Â  Â  # just numbers (fallback)

Â  Â  ]



Â  Â  for pattern in patterns:

Â  Â  Â  Â  pages_match = re.search(pattern, reading_text, re.IGNORECASE)

Â  Â  Â  Â  if pages_match:

Â  Â  Â  Â  Â  Â  page_str = pages_match.group(1)

Â  Â  Â  Â  Â  Â  # Handle various page formats like "3.1, 3.2, 3.3, and 3.4" or "10-15" or "10, 12, 14"

Â  Â  Â  Â  Â  Â  page_numbers = re.findall(r'[\d.]+', page_str)

Â  Â  Â  Â  Â  Â  # Convert to float for comparison

Â  Â  Â  Â  Â  Â  page_numbers = [float(p) for p in page_numbers]

Â  Â  Â  Â  Â  Â  break



Â  Â  # Set reading info based on extracted data

Â  Â  if assigned_author:

Â  Â  Â  Â  reading_info['author_last_name'] = assigned_author

Â  Â  else:

Â  Â  Â  Â  reading_info['author_last_name'] = ""



Â  Â  if page_numbers:

Â  Â  Â  Â  page_numbers.sort()

Â  Â  Â  Â  # Format page range for feedback - fix for issue 1

Â  Â  Â  Â  if len(page_numbers) == 1:

Â  Â  Â  Â  Â  Â  page_num = page_numbers[0]

Â  Â  Â  Â  Â  Â  if page_num.is_integer():

Â  Â  Â  Â  Â  Â  Â  Â  reading_info['page_range_expected'] = f"page {int(page_num)}"

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  reading_info['page_range_expected'] = f"page {page_num}"

Â  Â  Â  Â  elif len(page_numbers) == 2:

Â  Â  Â  Â  Â  Â  # Always use a hyphen for two pages, which are now sorted

Â  Â  Â  Â  Â  Â  if page_numbers[0].is_integer() and page_numbers[1].is_integer():

Â  Â  Â  Â  Â  Â  Â  Â  reading_info['page_range_expected'] = f"pages {int(page_numbers[0])}-{int(page_numbers[1])}"

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  reading_info['page_range_expected'] = f"pages {page_numbers[0]}-{page_numbers[1]}"

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  # Use a comma for lists of three or more pages

Â  Â  Â  Â  Â  Â  formatted_pages = []

Â  Â  Â  Â  Â  Â  for p in page_numbers:

Â  Â  Â  Â  Â  Â  Â  Â  if p.is_integer():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  formatted_pages.append(str(int(p)))

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  formatted_pages.append(str(p))

Â  Â  Â  Â  Â  Â  reading_info['page_range_expected'] = f"pages {', '.join(formatted_pages)}"

Â  Â  else:

Â  Â  Â  Â  reading_info['page_range_expected'] = "unspecified pages"



Â  Â  # ----------------------------------------------------

Â  Â  # D. Analyze citation presence based on updated rules

Â  Â  highest_max_reading_score = 2.0 # Default Minimum Score

Â  Â  best_citation_status_msg = f"NO CLEAR REFERENCE TO THE ASSIGNED READING WAS DETECTED. The minimum score of **2.0** applies."

Â  Â  detected_author = ""



Â  Â  # Only proceed with citation checking if we have an author

Â  Â  if reading_info['author_last_name']:

Â  Â  Â  Â  assigned_author_lower = reading_info['author_last_name'].lower()



Â  Â  Â  Â  # Check if the author name is present in the submission

Â  Â  Â  Â  author_present = re.search(r'\b' + re.escape(assigned_author_lower) + r'\b', submission_text.lower())



Â  Â  Â  Â  # Check if any of the page numbers are present in the submission

Â  Â  Â  Â  page_present = False

Â  Â  Â  Â  detected_pages = []

Â  Â  Â  Â  if page_numbers:

Â  Â  Â  Â  Â  Â  # If the assigned reading is a range, check for numbers within it

Â  Â  Â  Â  Â  Â  if len(page_numbers) == 2:

Â  Â  Â  Â  Â  Â  Â  Â  start_page = int(min(page_numbers))

Â  Â  Â  Â  Â  Â  Â  Â  end_page = int(max(page_numbers))

Â  Â  Â  Â  Â  Â  Â  Â  # Find all numbers in the submission that look like page citations

Â  Â  Â  Â  Â  Â  Â  Â  cited_pages = re.findall(r'(?:p|pg|page)s?\.?\s*(\d+)', submission_text, re.IGNORECASE)

Â  Â  Â  Â  Â  Â  Â  Â  for page_str in cited_pages:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cited_num = int(page_str)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if start_page <= cited_num <= end_page:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page_present = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detected_pages.append(page_str)

Â  Â  Â  Â  Â  Â  # Otherwise, check for specific pages listed

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  for page in page_numbers:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page_str = str(int(page)) if isinstance(page, float) and page.is_integer() else str(page)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  patterns = [

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r'\b' + re.escape(page_str) + r'\b',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r'\bp\.?\s*' + re.escape(page_str) + r'\b',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r'\bpage\s*' + re.escape(page_str) + r'\b',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r'\bpages?\s*' + re.escape(page_str) + r'\b'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for pattern in patterns:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if re.search(pattern, submission_text, re.IGNORECASE):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page_present = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detected_pages.append(page_str)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break # Found this page, move to the next assigned page

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if page_present and len(page_numbers) > 1:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # For multiple specific pages, finding one is enough.

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  # Determine score based on author and page presence

Â  Â  Â  Â  if author_present and page_present:

Â  Â  Â  Â  Â  Â  highest_max_reading_score = 4.0

Â  Â  Â  Â  Â  Â  best_citation_status_msg = f"Both the author ('{assigned_author}') and a relevant page number from the assigned reading were detected. Full credit awarded."

Â  Â  Â  Â  elif author_present:

Â  Â  Â  Â  Â  Â  highest_max_reading_score = 3.0

Â  Â  Â  Â  Â  Â  best_citation_status_msg = f"The author ('{assigned_author}') was mentioned, but no specific page number from the assigned reading was detected. Partial credit awarded."

Â  Â  Â  Â  elif page_present:

Â  Â  Â  Â  Â  Â  highest_max_reading_score = 3.5

Â  Â  Â  Â  Â  Â  best_citation_status_msg = f"A page number from the assigned reading was detected, but the author ('{assigned_author}') was not mentioned. Partial credit awarded."



Â  Â  Â  Â  # Check for incorrect author if the correct one wasn't found

Â  Â  Â  Â  if not author_present:

Â  Â  Â  Â  Â  Â  # Look for any capitalized name that might be an incorrect author

Â  Â  Â  Â  Â  Â  potential_authors = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', submission_text)

Â  Â  Â  Â  Â  Â  for potential_author in potential_authors:

Â  Â  Â  Â  Â  Â  Â  Â  # Skip common words that might be capitalized

Â  Â  Â  Â  Â  Â  Â  Â  if potential_author.lower() in ['the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', 'to', 'from', 'by']:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  Â  Â  Â  Â  # Check if this could be an author citation

Â  Â  Â  Â  Â  Â  Â  Â  if len(potential_author) > 3:Â  # Only consider names longer than 3 characters

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detected_author = potential_author

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if page_present:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  highest_max_reading_score = 3.5

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  best_citation_status_msg = f"A reference to '{potential_author}' with page number {', '.join(detected_pages)} was detected, but this does not match the assigned author ('{assigned_author}'). Partial credit awarded for the correct page reference."

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  highest_max_reading_score = 2.5

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  best_citation_status_msg = f"A reference to '{potential_author}' was detected, but this does not match the assigned author ('{assigned_author}'). Partial credit awarded."

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  else:

Â  Â  Â  Â  # No author found in reading text, so we can't check citations

Â  Â  Â  Â  highest_max_reading_score = 2.0

Â  Â  Â  Â  best_citation_status_msg = f"The assigned reading information did not specify an author. The minimum score of **2.0** applies."



Â  Â  max_reading_score = highest_max_reading_score

Â  Â  citation_status_msg = best_citation_status_msg



Â  Â  # --- GENERATE READING FEEDBACK BASED ON SCORE ---

Â  Â  if max_reading_score == 4.0:

Â  Â  Â  Â  reading_feedback_local = f"You successfully integrated concepts from the reading and provided a specific citation with a page number from {assigned_author}'s text ({reading_info['page_range_expected']}), earning full credit for this section"

Â  Â  elif max_reading_score == 3.5:

Â  Â  Â  Â  if detected_author:

Â  Â  Â  Â  Â  Â  # Fix for issue 2 - don't mention the specific wrong author

Â  Â  Â  Â  Â  Â  reading_feedback_local = f"You referenced page number(s) from the assigned reading, but cited the wrong author. Be sure to include the correct author to earn full credit"

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  reading_feedback_local = f"A page number from the assigned reading was detected, but the author was not mentioned. Include both the author and page number for full credit"

Â  Â  elif max_reading_score == 3.0:

Â  Â  Â  Â  reading_feedback_local = f"You mentioned the author ({assigned_author}), demonstrating engagement with the reading. However, you did not provide a specific page number from the assigned reading ({reading_info['page_range_expected']}) as required for higher credit. Include specific page references to earn full credit"

Â  Â  elif max_reading_score == 2.5:

Â  Â  Â  Â  # Fix for issue 2 - don't mention the specific wrong author

Â  Â  Â  Â  reading_feedback_local = f"You referenced the wrong author in your submission. Be sure to include the correct author and include a page number to earn more credit"

Â  Â  else:Â  # 2.0

Â  Â  Â  Â  if assigned_author:

Â  Â  Â  Â  Â  Â  reading_feedback_local = f"You successfully integrated concepts from the reading, but you did not provide a specific citation with a page number from {assigned_author}'s text ({reading_info['page_range_expected']}) as required for higher credit. You must include the author and a page number from the assigned reading to earn more than the minimum score"

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  reading_feedback_local = f"You successfully integrated concepts from the reading, but you did not provide a specific citation with a page number as required for higher credit. You must include the author and a page number from the assigned reading to earn more than the minimum score"



Â  Â  # ----------------------------------------------------



Â  Â  # Local scores - reading score is SET here, not by LLM

Â  Â  local_scores = {

Â  Â  Â  Â  'engagement_score': engagement_score,

Â  Â  Â  Â  'prompt_score': 0.0,

Â  Â  Â  Â  'reading_score': max_reading_score,Â  # DIRECTLY SET FROM PYTHON DETECTION

Â  Â  Â  Â  'key_terms_score': 0.0,

Â  Â  Â  Â  'video_score': 0.0

Â  Â  }

Â  Â  local_feedback = {

Â  Â  Â  Â  'engagement_feedback': engagement_analysis['feedback'],

Â  Â  Â  Â  'reading_feedback': reading_feedback_local,Â  # PYTHON-GENERATED FEEDBACK

Â  Â  Â  Â  'key_terms_fallback': f"LLM failed to provide key terms feedback. Detected terms: {detected_terms_str}"

Â  Â  }



Â  Â  # LLM scoring criteria - reading section is informational only

Â  Â  llm_scoring_criteria = f"""

SCORING Guidelines for LLM (10 points total - Reading is scored separately):

1. PROMPT ADHERENCE (Minimum 1.0 - 2.0): How well does the student address the entire prompt? (2.0 Maximum)

2. READING REFERENCE: **This section is scored separately by the system as {max_reading_score:.1f}. Do not provide a reading_score in your response.**

Â  Â  - Citation Status (for context): {citation_status_msg}

3. VIDEO REFERENCE (Minimum 2.0 - 4.0): How specific and relevant is the use of the assigned video material?

Â  Â  - Full credit (4.0) requires clear use of concepts demonstrated by specific examples or accurate summaries.

Â  Â  - **A specific timestamp is NOT required for a 4.0 score.**

4. KEY TERMS USAGE (Minimum 1.0 - 2.0): Did the student use at least one key term (from the detected list) in a way that demonstrates contextual understanding? (2.0 Maximum)

Â  Â  - FULL CREDIT (2.0) MUST BE AWARDED if ONE or more terms are used meaningfully.



Detected Key Terms to review for usage: "{detected_terms_str}"

"""



Â  Â  prompt_for_llm = f"""Grade this student discussion submission based ONLY on the following criteria. Reading Reference ({max_reading_score:.1f}) and Engagement ({engagement_score}) are scored separately by the system.



STUDENT: {student_first_name}



ASSIGNMENT CONTEXT:

Prompt: {discussion_prompt[:300]}...

Reading: {reading_text[:200]}...

Video: {video_text[:200]}...



{llm_scoring_criteria}



IMPORTANT: Provide SPECIFIC and ENCOURAGING feedback in the second person ("You", "Your").

**DO NOT include "reading_score" in your JSON response - it is handled separately.**



Respond with ONLY valid JSON. Omit any markdown fences (```json). Use floating point numbers rounded to the nearest 0.5.



{{

Â  Â  "prompt_score": "2.0",

Â  Â  "video_score": "4.0",

Â  Â  "key_terms_score": "2.0",

Â  Â  "prompt_feedback": "You successfully articulated how involuntary servitude was preserved and connected this theme to present-day issues.",

Â  Â  "video_feedback": "You clearly referenced the video context regarding convict leasing and the continuation of forced labor, demonstrating a strong grasp of the material.",

Â  Â  "key_terms_feedback": "Your contextual usage of key terms earns full credit, demonstrating clear understanding of the material.",

Â  Â  "general_feedback": "Your arguments were well-structured and demonstrated impressive critical thinking."

}}



SUBMISSION TEXT:

{submission_text[:1500]}

"""



Â  Â  # 2. API Scoring (if available)

Â  Â  api_results = {}



Â  Â  if api_key and validate_api_key(api_key):

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Prepare the API request

Â  Â  Â  Â  Â  Â  api_url = "https://openrouter.ai/api/v1/chat/completions"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  payload = {

Â  Â  Â  Â  Â  Â  Â  Â  "model": "google/gemini-2.5-flash-preview-09-2025",

Â  Â  Â  Â  Â  Â  Â  Â  "messages": [{"role": "user", "content": prompt_for_llm}],

Â  Â  Â  Â  Â  Â  Â  Â  "temperature": 0.1,

Â  Â  Â  Â  Â  Â  Â  Â  "response_format": {"type": "json_object"},

Â  Â  Â  Â  Â  Â  Â  Â  "max_tokens_for_reasoning": 512

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Make the API call with retries

Â  Â  Â  Â  Â  Â  response_text = robust_api_call(api_url, {}, payload, api_key=api_key)

Â  Â  Â  Â  Â  Â  api_results = robust_json_parsing(response_text)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Clean the API results

Â  Â  Â  Â  Â  Â  api_results = recursively_clean(api_results)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except ValueError as e:

Â  Â  Â  Â  Â  Â  # Handle specific ValueError exceptions (like authentication errors)

Â  Â  Â  Â  Â  Â  st.error(f"API Error: {str(e)}")

Â  Â  Â  Â  Â  Â  api_results = {}

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  # Handle other exceptions without exposing the full error message

Â  Â  Â  Â  Â  Â  st.error("An unexpected error occurred while calling the API. Please try again later.")

Â  Â  Â  Â  Â  Â  api_results = {}

Â  Â  else:

Â  Â  Â  Â  if not api_key:

Â  Â  Â  Â  Â  Â  st.warning("No API key provided. Using local scoring only.")

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  st.warning("Invalid API key format. Using local scoring only.")



Â  Â  # 3. Final Score Compilation

Â  Â  try:

Â  Â  Â  Â  local_scores['prompt_score'] = round_nearest_half(max(1.0, min(2.0, float(api_results.get("prompt_score", 1.0)))))

Â  Â  Â  Â  local_scores['video_score'] = round_nearest_half(max(2.0, min(4.0, float(api_results.get("video_score", 2.0)))))

Â  Â  Â  Â  local_scores['key_terms_score'] = round_nearest_half(max(1.0, min(2.0, float(api_results.get("key_terms_score", 1.0)))))

Â  Â  Â  Â  # reading_score already set from Python detection

Â  Â  except (ValueError, TypeError):

Â  Â  Â  Â  local_scores['prompt_score'] = 1.0

Â  Â  Â  Â  local_scores['video_score'] = 2.0

Â  Â  Â  Â  local_scores['key_terms_score'] = 1.0

Â  Â  Â  Â  # reading_score remains as set from Python detection



Â  Â  # Identify lowest scoring component for General Feedback

Â  Â  # For 15-point scale, we only consider 3 categories (excluding engagement)

Â  Â  if grading_scale == "15-point (3 categories)":

Â  Â  Â  Â  weighted_scores = {

Â  Â  Â  Â  Â  Â  "Prompt Adherence": local_scores['prompt_score'] / 2.0,

Â  Â  Â  Â  Â  Â  "Key Terms Usage": local_scores['key_terms_score'] / 2.0,

Â  Â  Â  Â  Â  Â  "Reading Reference": local_scores['reading_score'] / 4.0,

Â  Â  Â  Â  Â  Â  "Video Reference": local_scores['video_score'] / 4.0

Â  Â  Â  Â  }

Â  Â  else:Â  # 16-point (4 categories)

Â  Â  Â  Â  weighted_scores = {

Â  Â  Â  Â  Â  Â  "Prompt Adherence and Key Terms": (local_scores['prompt_score'] + local_scores['key_terms_score']) / 4.0,

Â  Â  Â  Â  Â  Â  "Reading Reference": local_scores['reading_score'] / 4.0,

Â  Â  Â  Â  Â  Â  "Video Reference": local_scores['video_score'] / 4.0,

Â  Â  Â  Â  Â  Â  "Discussion Engagement": local_scores['engagement_score'] / 4.0

Â  Â  Â  Â  }

Â  Â Â 

Â  Â  sorted_improvement = sorted(weighted_scores.items(), key=lambda item: item[1])

Â  Â  improvement_areas = [name for name, score in sorted_improvement if score < 1.0 and score > 0.0]



Â  Â  total = sum(local_scores.values())

Â  Â  total_score = round_nearest_half(total)



Â  Â  # Scale scores for 15-point system

Â  Â  if grading_scale == "15-point (3 categories)":

Â  Â  Â  Â  # Scale up the scores for 15-point scale (5 points per category)

Â  Â  Â  Â  # For 15-point scale, prompt and key terms are 2.5 points each

Â  Â  Â  Â  scaled_prompt_score = round_nearest_half(local_scores['prompt_score'] * 1.25)Â  # Scale from 2.0 to 2.5

Â  Â  Â  Â  scaled_key_terms_score = round_nearest_half(local_scores['key_terms_score'] * 1.25)Â  # Scale from 2.0 to 2.5

Â  Â  Â  Â  scaled_video_score = round_nearest_half(local_scores['video_score'] * 1.25)Â  # Scale from 4.0 to 5.0

Â  Â  Â  Â  scaled_reading_score = round_nearest_half(local_scores['reading_score'] * 1.25)Â  # Scale from 4.0 to 5.0

Â  Â  Â  Â  scaled_total = round_nearest_half(scaled_prompt_score + scaled_key_terms_score + scaled_video_score + scaled_reading_score)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # FIX: Keep scores as numeric values, not strings

Â  Â  Â  Â  final_grades = {

Â  Â  Â  Â  Â  Â  "prompt_score": scaled_prompt_score,Â  # Use scaled value

Â  Â  Â  Â  Â  Â  "key_terms_score": scaled_key_terms_score,Â  # Use scaled value

Â  Â  Â  Â  Â  Â  "video_score": scaled_video_score,Â  # Use scaled value

Â  Â  Â  Â  Â  Â  "reading_score": scaled_reading_score,Â  # Use scaled value

Â  Â  Â  Â  Â  Â  "engagement_score": 0,Â  # Not used in 15-point scale

Â  Â  Â  Â  Â  Â  "total_score": scaled_total,Â  # Use scaled total

Â  Â  Â  Â  }

Â  Â  else:

Â  Â  Â  Â  # FIX: Keep scores as numeric values, not strings

Â  Â  Â  Â  final_grades = {

Â  Â  Â  Â  Â  Â  "prompt_score": local_scores['prompt_score'],

Â  Â  Â  Â  Â  Â  "key_terms_score": local_scores['key_terms_score'],

Â  Â  Â  Â  Â  Â  "video_score": local_scores['video_score'],

Â  Â  Â  Â  Â  Â  "reading_score": local_scores['reading_score'],

Â  Â  Â  Â  Â  Â  "engagement_score": local_scores['engagement_score'],

Â  Â  Â  Â  Â  Â  "total_score": total_score,

Â  Â  Â  Â  }



Â  Â  final_grades["feedback"] = construct_final_feedback(api_results, local_scores, local_feedback, improvement_areas, student_first_name, grading_scale)



Â  Â  return final_grades



# ============================================

# STREAMLIT APP UI

# ============================================



# Set up the Streamlit page

st.set_page_config(

Â  Â  page_title="Discussion Grading Tool",

Â  Â  page_icon="ðŸ“š",

Â  Â  layout="wide",

Â  Â  initial_sidebar_state="expanded"

)



# Custom CSS for better styling with white text in dropdown

st.markdown("""

<style>

Â  Â  .main-header {

Â  Â  Â  Â  font-size: 2.5rem;

Â  Â  Â  Â  color: #1f77b4;

Â  Â  Â  Â  text-align: center;

Â  Â  }

Â  Â  .info-box {

Â  Â  Â  Â  background-color: #f0f2f6;

Â  Â  Â  Â  padding: 1rem;

Â  Â  Â  Â  border-radius: 0.5rem;

Â  Â  Â  Â  margin-bottom: 1rem;

Â  Â  }

Â  Â  .info-box h3 {

Â  Â  Â  Â  color: black !important;

Â  Â  }

Â  Â  .scale-info {

Â  Â  Â  Â  background-color: #e8f4f8;

Â  Â  Â  Â  padding: 1rem;

Â  Â  Â  Â  border-radius: 0.5rem;

Â  Â  Â  Â  margin-bottom: 1rem;

Â  Â  Â  Â  border-left: 5px solid #1f77b4;

Â  Â  Â  Â  color: black !important;

Â  Â  }

Â  Â  .scale-info h4 {

Â  Â  Â  Â  color: black !important;

Â  Â  }

Â  Â  .scale-info ul {

Â  Â  Â  Â  color: black !important;

Â  Â  }

Â  Â  .step-button {

Â  Â  Â  Â  color: black !important;

Â  Â  }

Â  Â  .download-button {

Â  Â  Â  Â  color: black !important;

Â  Â  }

Â  Â  /* Fix for the dropdown menu text color - ALL WHITE */

Â  Â  .stSelectbox > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSelectbox > div > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSelectbox label {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSelectbox > label > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSelectbox > label > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSelectbox > label > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSelectbox > label > div > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSidebar .stSelectbox {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSidebar .stSelectbox > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSidebar .stSelectbox > div > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSidebar .stSelectbox label {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSidebar .stSelectbox > label > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSidebar .stSelectbox > label > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSidebar .stSelectbox > label > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  .stSidebar .stSelectbox > label > div > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  /* Additional CSS to ensure dropdown options are visible */

Â  Â  div[data-baseweb="select"] {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  div[data-baseweb="select"] > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  div[data-baseweb="select"] > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  div[data-baseweb="select"] > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  div[data-baseweb="select"] > div > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  div[data-baseweb="select"] div[role="listbox"] {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  div[data-baseweb="select"] div[role="listbox"] > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  div[data-baseweb="select"] div[role="listbox"] > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  div[data-baseweb="select"] div[role="listbox"] > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  /* Fix for selected text in dropdown - WHITE */

Â  Â  div[data-baseweb="select"] div[role="listbox"] > div[data-selected="true"] {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  /* Fix for hover state in dropdown - WHITE */

Â  Â  div[data-baseweb="select"] div[role="listbox"] > div:hover {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  /* Fix for dropdown arrow */

Â  Â  svg[data-testid="stSelectboxDropdownIcon"] {

Â  Â  Â  Â  fill: white !important;

Â  Â  }

Â  Â  /* Fix for the selected value display - WHITE */

Â  Â  div[data-baseweb="select"] > div > div > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

Â  Â  /* Additional fix for the selected value - WHITE */

Â  Â  div[data-baseweb="select"] > div > div > div > div > div > div {

Â  Â  Â  Â  color: white !important;

Â  Â  }

</style>

""", unsafe_allow_html=True)



# Header

st.markdown('<h1 class="main-header">ðŸ“š Automated Discussion Grading Tool</h1>', unsafe_allow_html=True)

st.markdown("Upload your CSV and DOCX files to grade student discussions.")



# Sidebar for configuration

st.sidebar.header("Configuration")



# Try to get API key from Streamlit secrets first, then from environment variable, then from user input

api_key = None

api_key_source = "Not set"



# Try Streamlit secrets

try:

Â  Â  api_key = st.secrets["OPENROUTER_API_KEY"]

Â  Â  api_key_source = "Streamlit secrets"

Â  Â  # Clean the API key to remove any extra quotes or whitespace

Â  Â  api_key = api_key.strip().strip('"').strip("'")

Â  Â  st.sidebar.success("API key loaded from Streamlit secrets")

except (KeyError, FileNotFoundError):

Â  Â  pass



# Try environment variable if not found in secrets

if not api_key:

Â  Â  api_key = os.getenv("OPENROUTER_API_KEY")

Â  Â  if api_key:

Â  Â  Â  Â  api_key_source = "Environment variable"

Â  Â  Â  Â  # Clean the API key to remove any extra quotes or whitespace

Â  Â  Â  Â  api_key = api_key.strip().strip('"').strip("'")

Â  Â  Â  Â  st.sidebar.success("API key loaded from environment variable")



# Fallback to user input

if not api_key:

Â  Â  api_key_input = st.sidebar.text_input("OpenRouter API Key", type="password", help="Enter your OpenRouter API key")

Â  Â  if api_key_input:

Â  Â  Â  Â  api_key = api_key_input.strip()

Â  Â  Â  Â  api_key_source = "User input"

else:

Â  Â  # If we already have an API key, show a masked input

Â  Â  st.sidebar.text_input("OpenRouter API Key", type="password", value="â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢", disabled=True)



# Display API key status

if api_key:

Â  Â  st.sidebar.info(f"API key source: {api_key_source}")

Â  Â Â 

Â  Â  # Validate the API key format

Â  Â  if api_key.startswith("sk-or-v1-") and len(api_key) > 20:

Â  Â  Â  Â  st.sidebar.success("API key format appears valid")

Â  Â  else:

Â  Â  Â  Â  st.sidebar.error("Invalid API key format. OpenRouter API keys should start with 'sk-or-v1-' and be at least 20 characters long.")

else:

Â  Â  st.sidebar.error("No API key provided. Please enter your API key to continue.")



st.sidebar.markdown("Get your API key from [OpenRouter](https://openrouter.ai/)")



# Add grading scale selector

grading_scale = st.sidebar.selectbox(

Â  Â  "Select Grading Scale",

Â  Â  ["15-point (3 categories)", "16-point (4 categories)"],

Â  Â  index=0,

Â  Â  help="Choose between the 15-point scale (3 categories) or the 16-point scale (4 categories)"

)



# Display information about the selected grading scale

if grading_scale == "15-point (3 categories)":

Â  Â  st.sidebar.markdown("""

Â  Â  <div class="scale-info">

Â  Â  <h4>15-Point Scale (3 Categories)</h4>

Â  Â  <ul>

Â  Â  Â  Â  <li>Prompt Adherence (2.5 points)</li>

Â  Â  Â  Â  <li>Key Terms (2.5 points)</li>

Â  Â  Â  Â  <li>Reading Reference (5.0 points)</li>

Â  Â  Â  Â  <li>Video Reference (5.0 points)</li>

Â  Â  </ul>

Â  Â  </div>

Â  Â  """, unsafe_allow_html=True)

else:

Â  Â  st.sidebar.markdown("""

Â  Â  <div class="scale-info">

Â  Â  <h4>16-Point Scale (4 Categories)</h4>

Â  Â  <ul>

Â  Â  Â  Â  <li>Prompt Adherence and Key Terms (4.0 points)</li>

Â  Â  Â  Â  <li>Reading Reference (4.0 points)</li>

Â  Â  Â  Â  <li>Video Reference (4.0 points)</li>

Â  Â  Â  Â  <li>Discussion Engagement (4.0 points)</li>

Â  Â  </ul>

Â  Â  </div>

Â  Â  """, unsafe_allow_html=True)



st.sidebar.markdown("---")



# Main content area

col1, col2 = st.columns(2)



with col1:

Â  Â  st.markdown('<div class="info-box"><h3 class="step-button">Step 1: Upload Files</h3></div>', unsafe_allow_html=True)

Â  Â  csv_file = st.file_uploader("Upload CSV file with student submissions", type=['csv'], key="csv_uploader")

Â  Â  st.markdown("The CSV should contain columns for student names, initial posts, and replies.")

Â  Â Â 

Â  Â  # Add debugging information

Â  Â  if csv_file is not None:

Â  Â  Â  Â  st.success(f"CSV file uploaded: {csv_file.name}")

Â  Â  else:

Â  Â  Â  Â  st.info("Please upload a CSV file to continue.")



with col2:

Â  Â  st.markdown('<div class="info-box"><h3 class="step-button">Step 2: Upload Lesson Plan</h3></div>', unsafe_allow_html=True)

Â  Â  docx_file = st.file_uploader("Upload DOCX lesson plan", type=['docx'], key="docx_uploader")

Â  Â  st.markdown("The lesson plan should contain discussion prompts, reading assignments, and key terms.")

Â  Â Â 

Â  Â  # Add debugging information

Â  Â  if docx_file is not None:

Â  Â  Â  Â  st.success(f"DOCX file uploaded: {docx_file.name}")

Â  Â  else:

Â  Â  Â  Â  st.info("Please upload a DOCX file to continue.")



# Process files when both are uploaded

if csv_file and docx_file:

Â  Â  st.markdown("---")

Â  Â  st.markdown('<div class="info-box"><h3 class="step-button">Step 3: Process Files</h3></div>', unsafe_allow_html=True)

Â  Â Â 

Â  Â  if st.button("ðŸš€ Process Files", type="primary"):

Â  Â  Â  Â  if not api_key:

Â  Â  Â  Â  Â  Â  st.error("Please enter your OpenRouter API key in the sidebar.")

Â  Â  Â  Â  elif not validate_api_key(api_key):

Â  Â  Â  Â  Â  Â  st.error("Invalid API key format. OpenRouter API keys should start with 'sk-or-v1-' and be at least 20 characters long.")

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  with st.spinner("Processing files... This may take a few minutes."):

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Reset file pointers to the beginning

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  csv_file.seek(0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  docx_file.seek(0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Read files

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  csv_content = csv_file.read()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  docx_content = docx_file.read()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add a progress bar

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_bar = st.progress(0)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("Parsing lesson plan...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Call your existing functions

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  discussion_prompt, reading_text, video_text, key_terms, objective, key_concepts = parse_lesson_plan_comprehensive(docx_content)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.progress(0.25)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("Reading CSV file...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Process CSV

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  csv_io = io.StringIO(csv_content.decode('utf-8'))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rows = list(csv.DictReader(csv_io))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except UnicodeDecodeError:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("Error decoding CSV file. Please ensure it's saved in UTF-8 format.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error reading CSV file: {str(e)}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Check if we got any rows

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not rows:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("No data found in CSV file. Please check the file format.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Store original columns

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_columns = list(rows[0].keys()) if rows else []

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Display extracted information

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.progress(0.5)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("Analyzing submissions...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Create a dataframe to store results

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Process each student submission

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_students = len(rows)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, row in enumerate(rows):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  student_name = row.get('Name', row.get('Student Name', row.get('Username', 'Unknown')))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  initial_post = row.get('Initial Post', row.get('Initial Posts', ''))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  replies = [row.get(f'Reply {j}', '') for j in range(1, 4) if row.get(f'Reply {j}', '')]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Extract first name for personalization

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  student_first_name = student_name.split()[0] if student_name else "Student"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Grade the submission

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  grade_result = grade_submission_with_retries(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  initial_post,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reading_text,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key_terms,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  discussion_prompt,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  student_first_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video_text,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  replies,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  api_key,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  grading_scale

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add to results - preserving all original columns

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result_row = row.copy()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result_row.update(grade_result)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.append(result_row)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Update progress

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress = (i + 1) / total_students

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.progress(0.5 + progress * 0.5)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Create dataframe from results

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df = pd.DataFrame(results)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Display results

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("Processing complete!")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown('<div class="info-box"><h3>Grading Results</h3></div>', unsafe_allow_html=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Show statistics

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ðŸ“Š Grade Statistics")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  col1, col2, col3 = st.columns(3)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col1:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  avg_score = df['total_score'].mean()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Average Score", f"{avg_score:.2f}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  median_score = df['total_score'].median()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Median Score", f"{median_score:.2f}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col3:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_score = df['total_score'].min()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_score = df['total_score'].max()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Score Range", f"{min_score:.1f} - {max_score:.1f}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Show score distribution

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ðŸ“ˆ Score Distribution")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig = px.histogram(df, x="total_score", nbins=20, title="Distribution of Scores")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.plotly_chart(fig, use_container_width=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Show detailed results

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ðŸ“‹ Detailed Results")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(df)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Download button

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  csv_output = df.to_csv(index=False)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Get the original filename and add GRADED_ prefix

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_filename = csv_file.name

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  base_filename = os.path.splitext(original_filename)[0]Â  # Remove .csv extension

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  graded_filename = f"GRADED_{base_filename}.csv"



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown('<div class="download-button">', unsafe_allow_html=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="ðŸ“¥ Download Graded CSV",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=csv_output,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=graded_filename,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown('</div>', unsafe_allow_html=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"An error occurred: {str(e)}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import traceback

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(traceback.format_exc())
